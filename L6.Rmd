---
title: "Linear Methods for Classification"
output:
  pdf_document: default
  html_document: default
---

---
title: "Nonlinear Methods: Splines and GAM"
output: pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(caret)
library(glmnet)
library(MASS)
library(e1071)
library(mlbench)
library(pROC)
library(AppliedPredictiveModeling)
```

We use the Pima Indians Diabetes Database for illustration. The data contain 768 observations and 9 variables. The outcome is a binary variable `diabetes`. We start from some simple visualization of the data.

```{r}
data(PimaIndiansDiabetes)
dat <- PimaIndiansDiabetes

transparentTheme(trans = .4)
featurePlot(x = dat[, 1:8], 
            y = dat$diabetes,
            scales = list(x=list(relation="free"), 
                        y=list(relation="free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))
```

The data is divided into two parts (training and test). 
```{r}
set.seed(1)
rowTrain <- createDataPartition(y = dat$diabetes,
                                p = 0.75,
                                list = FALSE)
```

## Logistic regression

```{r}
glm.fit <- glm(diabetes~., 
               data = dat, 
               subset = rowTrain, 
               family = binomial)

contrasts(dat$diabetes)
```

We first consider the Bayes classifier (cutoff 0.5) and evaluate its performance on the test data.
```{r}
test.pred.prob  <- predict(glm.fit, newdata = dat[-rowTrain,],
                           type = "response")
test.pred <- rep("neg", length(test.pred.prob))
test.pred[test.pred.prob>0.5] <- "pos"


# confusionMatrix(data = as.factor(test.pred),
#                 reference = dat$diabetes[-rowTrain])

confusionMatrix(data = as.factor(test.pred),
                reference = dat$diabetes[-rowTrain],
                positive = "pos")
```

We then plot the test ROC curve. You may (or may not) also consider to add a smoothed ROC curve.
```{r}
roc.glm <- roc(dat$diabetes[-rowTrain], test.pred.prob)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)
```

We can also fit a logistic regression using caret. This is to compare the cross-valiation performance with other models, rather than tuning the model.
```{r}
# Using caret
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(1)
model.glm <- train(x = dat[rowTrain,1:8],
                   y = dat$diabetes[rowTrain],
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl)
```

Regularized logistic regression can be fitted using `glmnet'. We use the `train` function to select the optimal tuning parameters.

```{r}
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 6),
                        .lambda = exp(seq(-6, -2, length = 20)))
set.seed(1)
model.glmn <- train(x = dat[rowTrain,1:8],
                    y = dat$diabetes[rowTrain],
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)

plot(model.glmn, xTrans = function(x) log(x))   
```


## Discriminant analysis

### LDA

We use the function `lda` in library `MASS` to conduct LDA.
```{r}
library(MASS)

lda.fit <- lda(diabetes~., data = dat,
               subset = rowTrain)
plot(lda.fit)
```

Evaluate the test set performance using ROC.
```{r}
lda.pred <- predict(lda.fit, newdata = dat[-rowTrain,])
head(lda.pred$posterior)

roc.lda <- roc(dat$diabetes[-rowTrain], lda.pred$posterior[,2], 
               levels = c("neg", "pos"))

plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)
```

Using caret:
```{r}
set.seed(1)
model.lda <- train(x = dat[rowTrain,1:8],
                   y = dat$diabetes[rowTrain],
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)
```

### QDA

```{r}
set.seed(1)
model.qda <- train(x = dat[rowTrain,1:8],
                   y = dat$diabetes[rowTrain],
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl)

# use qda() in MASS
qda.fit <- qda(diabetes~., data = dat,
               subset = rowTrain)

qda.pred <- predict(qda.fit, newdata = dat[-rowTrain,])
head(qda.pred$posterior)
```

### Naive Bayes

```{r, warning=FALSE}
set.seed(1)

nbGrid <- expand.grid(usekernel = c(FALSE,TRUE),
                      fL = 1, 
                      adjust = seq(0,5,by = 1))

model.nb <- train(x = dat[rowTrain,1:8],
                  y = dat$diabetes[rowTrain],
                  method = "nb",
                  tuneGrid = nbGrid,
                  metric = "ROC",
                  trControl = ctrl)

plot(model.nb)
```

### KNN

```{r, warning=FALSE}
set.seed(1)

model.knn <- train(x = dat[rowTrain,1:8],
                   y = dat$diabetes[rowTrain],
                   method = "knn",
                   preProcess = c("center","scale"),
                   tuneGrid = data.frame(k = seq(1,200,by=5)),
                   trControl = ctrl)

ggplot(model.knn)
```

GLM, Regularized GLM and LDA have relatively good performance.
```{r}
res <- resamples(list(GLM = model.glm, GLMNET = model.glmn, 
                      LDA = model.lda, QDA = model.qda,
                      NB = model.nb, KNN = model.knn))
summary(res)
```

Now let's look at the test set performance.
```{r, warning=FALSE}
lda.pred <- predict(model.lda, newdata = dat[-rowTrain,], type = "prob")[,2]
glm.pred <- predict(model.glm, newdata = dat[-rowTrain,], type = "prob")[,2]
glmn.pred <- predict(model.glmn, newdata = dat[-rowTrain,], type = "prob")[,2]
nb.pred <- predict(model.nb, newdata = dat[-rowTrain,], type = "prob")[,2]
qda.pred <- predict(model.qda, newdata = dat[-rowTrain,], type = "prob")[,2]
knn.pred <- predict(model.knn, newdata = dat[-rowTrain,], type = "prob")[,2]


roc.lda <- roc(dat$diabetes[-rowTrain], lda.pred)
roc.glm <- roc(dat$diabetes[-rowTrain], glm.pred)
roc.glmn <- roc(dat$diabetes[-rowTrain], glmn.pred)
roc.nb <- roc(dat$diabetes[-rowTrain], nb.pred)
roc.qda <- roc(dat$diabetes[-rowTrain], qda.pred)
roc.knn <- roc(dat$diabetes[-rowTrain], knn.pred)

auc <- c(roc.glm$auc[1], roc.glmn$auc[1], roc.lda$auc[1],
         roc.qda$auc[1], roc.nb$auc[1], roc.knn$auc[1])

plot(roc.glm, legacy.axes = TRUE)
plot(roc.glmn, col = 2, add = TRUE)
plot(roc.lda, col = 3, add = TRUE)
plot(roc.qda, col = 4, add = TRUE)
plot(roc.nb, col = 5, add = TRUE)
plot(roc.knn, col = 6, add = TRUE)
modelNames <- c("glm","glmn","lda","qda","nb","knn")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:6, lwd = 2)
```